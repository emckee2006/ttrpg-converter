# M1.5: Database-Driven Architecture

**Author:** Gemini
**Date:** 2024-07-25
**Status:** Adopted

## 1. Overview

This document outlines a foundational architectural shift for the TTRPG Converter. The original plan (M1) envisioned an in-memory processing model, which is insufficient for the project's long-term goals of handling large compendia, parallel processing, and complex asset management.

This new architecture, to be implemented before proceeding with the core engine tasks of M2, introduces a persistent, database-driven approach using **RavenDB Embedded**. This will provide a robust, scalable, and performant foundation for all future development.

## 2. Core Problem

The initial in-memory-per-run strategy presents several critical issues:

-   **Performance Bottleneck:** Re-processing all source compendia on every conversion run is not scalable.
-   **Concurrency Unsafe:** It provides no safe mechanism for parallel tasks to coordinate, leading to race conditions and data corruption.
-   **High Memory Usage:** Requires the entire dataset to be loaded into RAM.
-   **Complex State Management:** Requires developers to write and maintain complex, error-prone logic for caching and state management.

## 3. The RavenDB Embedded Solution

To address these issues, we will use **RavenDB Embedded** as the central data store for both the persistent compendium cache and the transient state of a conversion run.

### 3.1. Why RavenDB Embedded?

-   **Document-Oriented:** Perfectly suited for storing and indexing complex JSON-like documents (`CompendiumItem`, `AssetMapping`).
-   **ACID Transactions:** Guarantees data integrity and eliminates the risk of a corrupt cache.
-   **High-Performance, Indexed Lookups:** Allows for fast queries directly on disk, minimizing memory usage.
-   **Built-in Concurrency Control:** The session-based unit of work provides a robust, industry-standard solution for managing parallel reads and writes, eliminating the need for manual locking.

### 3.2. The `update-compendium` Command

A new CLI command will be created to manage the persistent compendium cache. This command will use a **Producer-Consumer Pattern** for optimal performance.

**Workflow:**

1.  **Producer (Synchronous Extraction):** A single thread iterates through all LevelDB packs, synchronously running the `foundry-extract.ps1` script for each. As each pack is extracted to a temporary directory, its path is posted to a `System.Threading.Channels.Channel<string>`.
2.  **Consumers (Parallel Processing & Insertion):** A pool of worker threads consumes the directory paths from the channel. Each thread is responsible for:
    a.  Processing all JSON files in its assigned directory into `CompendiumItem` objects.
    b.  Opening a new `IDocumentSession` with RavenDB.
    c.  Storing the new `CompendiumItem` documents in the database.
    d.  Deleting the temporary directory.
3.  **Conflict Resolution (Final Step):** After all consumers are finished, a single process runs to resolve duplicates. It groups all items in the database by `Type` and `Name`, identifies a "winner" for each group based on source priority, and performs a batch update to set the `IsPrimary` flag to `true` for the winner and `false` for all others.

### 3.3. The Conversion Process

1.  **Permanent Cache (`CompendiumItems` Collection):** The `update-compendium` command populates a single, permanent collection in the `compendium.ravendb` database. This collection holds every item from every source, with duplicates preserved and flagged.
2.  **Temporary Collections (`ConversionRun_*`):** During a `convert-campaign` run, a new, temporary collection is created to hold all world-specific entities (custom items, actors, scenes). This isolates the run's state and allows for easy cleanup.
3.  **The `CompendiumManager`:** This service now acts as a lightweight consumer of the cache. Its methods (`FindEntity`, `FindAllCandidates`) perform fast, indexed queries directly against the RavenDB database.

## 4. Implementation Plan

1.  **Add RavenDB.Embedded Dependency:** Add the NuGet package to the `TTRPGConverter.Core.csproj` file.
2.  **Update `CompendiumItem` Model:** Add a `public bool IsPrimary { get; set; }` property.
3.  **Create `CompendiumItem_Index`:** Create a new class to define the RavenDB index on `Type`, `Name`, and `IsPrimary`.
4.  **Refactor `UpdateCompendiumCommand`:** Implement the full producer-consumer pipeline for extracting, processing, and storing all compendium items.
5.  **Implement `ResolveConflictsAsync` Method:** Add the final deduplication and flagging logic to the `UpdateCompendiumCommand`.
6.  **Refactor `CompendiumManager`:** Rewrite the data lookup methods to query the RavenDB cache directly.
7.  **Integrate into `ConvertCampaignCommand`:** Inject the `CompendiumManager` and use it to look up compendium data during the conversion process.

This architecture provides a scalable, reliable, and high-performance foundation for the entire conversion process.
